#!/usr/bin/env python

# Copyright (c) 2018 NVIDIA Corporation. All rights reserved.
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode

"""
This file starts a ROS node to run DOPE, 
listening to an image topic and publishing poses.
"""

from __future__ import print_function

import cv2
# import imageio
import message_filters
import numpy as np
import resource_retriever
import rospy
import tf.transformations
import trimesh
from PIL import Image
from PIL import ImageDraw
from autolab_core import RigidTransform
from cv_bridge import CvBridge
from dope.cfg import DopeConfig
from dope.inference.cuboid import Cuboid3d
from dope.inference.cuboid_pnp_solver import CuboidPNPSolver
from dope.inference.detector import ModelData, ObjectDetector
from dynamic_reconfigure.server import Server
from geometry_msgs.msg import PoseStamped
from meshrender import Scene, MaterialProperties, SceneObject, VirtualCamera, SceneViewer
from perception import CameraIntrinsics, RenderMode
from sensor_msgs.msg import CameraInfo, Image as ImageSensor_msg
from std_msgs.msg import String, Empty, ColorRGBA
from vision_msgs.msg import Detection3D, Detection3DArray, ObjectHypothesisWithPose
from visualization_msgs.msg import Marker, MarkerArray


class Draw(object):
    """Drawing helper class to visualize the neural network output"""

    def __init__(self, im):
        """
        :param im: The image to draw in.
        """
        self.draw = ImageDraw.Draw(im)

    def draw_line(self, point1, point2, line_color, line_width=2):
        """Draws line on image"""
        if point1 is not None and point2 is not None:
            self.draw.line([point1, point2], fill=line_color, width=line_width)

    def draw_dot(self, point, point_color, point_radius):
        """Draws dot (filled circle) on image"""
        if point is not None:
            xy = [
                point[0] - point_radius,
                point[1] - point_radius,
                point[0] + point_radius,
                point[1] + point_radius
            ]
            self.draw.ellipse(xy,
                              fill=point_color,
                              outline=point_color
                              )

    def draw_cube(self, points, color=(255, 0, 0)):
        """
        Draws cube with a thick solid line across
        the front top edge and an X on the top face.
        """

        # draw front
        self.draw_line(points[0], points[1], color)
        self.draw_line(points[1], points[2], color)
        self.draw_line(points[3], points[2], color)
        self.draw_line(points[3], points[0], color)

        # draw back
        self.draw_line(points[4], points[5], color)
        self.draw_line(points[6], points[5], color)
        self.draw_line(points[6], points[7], color)
        self.draw_line(points[4], points[7], color)

        # draw sides
        self.draw_line(points[0], points[4], color)
        self.draw_line(points[7], points[3], color)
        self.draw_line(points[5], points[1], color)
        self.draw_line(points[2], points[6], color)

        # draw dots
        self.draw_dot(points[0], point_color=color, point_radius=4)
        self.draw_dot(points[1], point_color=color, point_radius=4)

        # draw x on the top
        self.draw_line(points[0], points[5], color)
        self.draw_line(points[1], points[4], color)


class DopeNode(object):
    """ROS node that listens to image topic, runs DOPE, and publishes DOPE results"""

    def __init__(self):
        self.pubs = {}
        self.models = {}
        self.pnp_solvers = {}
        self.pub_dimension = {}
        self.draw_colors = {}
        self.dimensions = {}
        self.class_ids = {}
        self.model_transforms = {}
        self.mesh_urls = {}
        self.meshes = {}
        self.mesh_scales = {}
        self.cv_bridge = CvBridge()

        self.input_is_rectified = rospy.get_param('~input_is_rectified', True)
        self.downscale_height = rospy.get_param('~downscale_height', 500)

        # post processing
        self.depth_tolerance = rospy.get_param('~depth_tolerance', 0.30)
        self.min_valid_points_ratio = rospy.get_param('~min_valid_points_ratio', 0.01)

        self.config_detect = lambda: None
        self.config_detect.mask_edges = 1
        self.config_detect.mask_faces = 1
        self.config_detect.vertex = 1
        self.config_detect.threshold = 0.5
        self.config_detect.softmax = 1000
        self.config_detect.thresh_angle = rospy.get_param('~thresh_angle', 0.5)
        self.config_detect.thresh_map = rospy.get_param('~thresh_map', 0.01)
        self.config_detect.sigma = rospy.get_param('~sigma', 3)
        self.config_detect.thresh_points = rospy.get_param("~thresh_points", 0.1)

        self.config_detect.wait_for_trigger = False
        self.config_detect.has_been_triggered = False
        self.dynconfig_server = Server(DopeConfig, self.dynconfig_callback)

        # Subscribe to trigger messages
        self.sub_trigger = rospy.Subscriber("~trigger", Empty, self.trigger_callback)

        # For each object to detect, load network model, create PNP solver, and start ROS publishers
        for model, weights_url in rospy.get_param('~weights').iteritems():
            self.models[model] = \
                ModelData(
                    model,
                    resource_retriever.get_filename(weights_url, use_protocol=False)
                )
            self.models[model].load_net_model()

            try:
                M = np.array(rospy.get_param('~model_transforms')[model], dtype='float64')
                self.model_transforms[model] = tf.transformations.quaternion_from_matrix(M)
            except KeyError:
                self.model_transforms[model] = np.array([0.0, 0.0, 0.0, 1.0], dtype='float64')

            try:
                self.mesh_scales[model] = rospy.get_param('~mesh_scales')[model]
            except KeyError:
                self.mesh_scales[model] = 1.0

            try:
                self.mesh_urls[model] = rospy.get_param('~meshes')[model]
                mesh_path = resource_retriever.get_filename(self.mesh_urls[model], use_protocol=False)
                mesh = trimesh.load_mesh(mesh_path)

                if type(mesh) is trimesh.base.Trimesh:
                    mesh.apply_scale(self.mesh_scales[model])
                    self.meshes[model] = mesh
                else:
                    rospy.logerr('The mesh "{0!s}" was loaded as class {0!s} (expected: {0!s}).'.format(
                        mesh_path, type(mesh), trimesh.base.Trimesh))
                    rospy.logerr('This probably means that it consists of multiple geometries; post processing will'
                                 + ' not work for this class. Try loading an STL file instead.')
            except KeyError:
                pass

            self.draw_colors[model] = tuple(rospy.get_param("~draw_colors")[model])
            self.dimensions[model] = tuple(rospy.get_param("~dimensions")[model])
            self.class_ids[model] = rospy.get_param("~class_ids")[model]

            self.pnp_solvers[model] = \
                CuboidPNPSolver(
                    model,
                    cuboid3d=Cuboid3d(rospy.get_param('~dimensions')[model])
                )
            self.pubs[model] = \
                rospy.Publisher(
                    '{}/pose_{}'.format(rospy.get_param('~topic_publishing'), model),
                    PoseStamped,
                    queue_size=10
                )
            self.pub_dimension[model] = \
                rospy.Publisher(
                    '{}/dimension_{}'.format(rospy.get_param('~topic_publishing'), model),
                    String,
                    queue_size=10
                )

        # Start ROS publishers
        self.pub_rgb_dope_points = \
            rospy.Publisher(
                rospy.get_param('~topic_publishing') + "/rgb_points",
                ImageSensor_msg,
                queue_size=10
            )
        self.pub_detections = \
            rospy.Publisher(
                '~detected_objects',
                Detection3DArray,
                queue_size=10
            )
        self.pub_markers = \
            rospy.Publisher(
                '~markers',
                MarkerArray,
                queue_size=10
            )

        # Start ROS subscriber
        image_sub = message_filters.Subscriber(
            rospy.get_param('~topic_camera'),
            ImageSensor_msg
        )
        info_sub = message_filters.Subscriber(
            rospy.get_param('~topic_camera_info'),
            CameraInfo
        )
        try:
            depth_sub = message_filters.Subscriber(
                rospy.get_param('~topic_depth'),
                ImageSensor_msg
            )
            ts = message_filters.TimeSynchronizer([image_sub, info_sub, depth_sub], 1)
            ts.registerCallback(self.image_callback)
        except KeyError:
            # parameter topic_depth is not set, so we'll run without depth image based post processing
            ts = message_filters.TimeSynchronizer([image_sub, info_sub], 1)
            ts.registerCallback(self.image_callback)

        print("Running DOPE...  (Listening to camera topic: '{}')".format(rospy.get_param('~topic_camera')))
        print("Ctrl-C to stop")

    def dynconfig_callback(self, config, _):
        """Dynamic reconfigure callback"""
        rospy.loginfo("dynamic_reconfigure: wait_for_trigger: " + str(config.wait_for_trigger))
        self.config_detect.wait_for_trigger = config.wait_for_trigger
        return config

    def trigger_callback(self, _):
        rospy.loginfo("got triggered!")
        self.config_detect.has_been_triggered = True

    def image_callback(self, image_msg, camera_info, depth_msg=None):
        """Image callback"""
        if self.config_detect.wait_for_trigger:
            if not self.config_detect.has_been_triggered:
                return
        self.config_detect.has_been_triggered = False

        img = self.cv_bridge.imgmsg_to_cv2(image_msg, "rgb8")
        if depth_msg:
            depth_img = self.cv_bridge.imgmsg_to_cv2(depth_msg, "32FC1")

        # Update camera matrix and distortion coefficients
        if self.input_is_rectified:
            P = np.matrix(camera_info.P, dtype='float64')
            P.resize((3, 4))
            camera_matrix = P[:, :3]
            dist_coeffs = np.zeros((4, 1))
        else:
            camera_matrix = np.matrix(camera_info.K, dtype='float64')
            camera_matrix.resize((3, 3))
            dist_coeffs = np.matrix(camera_info.D, dtype='float64')
            dist_coeffs.resize((len(camera_info.D), 1))

        # Downscale image if necessary
        height, width, _ = img.shape
        scaling_factor = float(self.downscale_height) / height
        if scaling_factor < 1.0:
            camera_matrix[:2] *= scaling_factor
            img = cv2.resize(img, (int(scaling_factor * width), int(scaling_factor * height)))
            if depth_msg:
                depth_img = cv2.resize(depth_img, (int(scaling_factor * width), int(scaling_factor * height)))

        for m in self.models:
            self.pnp_solvers[m].set_camera_intrinsic_matrix(camera_matrix)
            self.pnp_solvers[m].set_dist_coeffs(dist_coeffs)

        # Copy and draw image
        img_copy = img.copy()
        im = Image.fromarray(img_copy)
        draw = Draw(im)

        detection_array = Detection3DArray()
        detection_array.header = image_msg.header

        for m in self.models:
            # Detect object
            results = ObjectDetector.detect_object_in_image(
                self.models[m].net,
                self.pnp_solvers[m],
                img,
                self.config_detect
            )

            # Publish pose and overlay cube on image
            for i_r, result in enumerate(results):
                if result["location"] is None:
                    continue
                loc = result["location"]
                ori = result["quaternion"]

                # transform orientation
                transformed_ori = tf.transformations.quaternion_multiply(ori, self.model_transforms[m])

                # rotate bbox dimensions if necessary
                # (this only works properly if model_transform is in 90 degree angles)
                dims = rotate_vector(vector=self.dimensions[m], quaternion=self.model_transforms[m])
                dims = np.absolute(dims)
                dims = tuple(dims)

                pose_msg = PoseStamped()
                pose_msg.header = image_msg.header
                CONVERT_SCALE_CM_TO_METERS = 100
                pose_msg.pose.position.x = loc[0] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.position.y = loc[1] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.position.z = loc[2] / CONVERT_SCALE_CM_TO_METERS
                pose_msg.pose.orientation.x = transformed_ori[0]
                pose_msg.pose.orientation.y = transformed_ori[1]
                pose_msg.pose.orientation.z = transformed_ori[2]
                pose_msg.pose.orientation.w = transformed_ori[3]

                # Publish
                self.pubs[m].publish(pose_msg)
                self.pub_dimension[m].publish(str(dims))

                # Add to Detection3DArray
                detection = Detection3D()
                hypothesis = ObjectHypothesisWithPose()
                hypothesis.id = self.class_ids[result["name"]]
                hypothesis.score = result["score"]
                hypothesis.pose.pose = pose_msg.pose
                detection.results.append(hypothesis)
                detection.bbox.center = pose_msg.pose
                detection.bbox.size.x = dims[0] / CONVERT_SCALE_CM_TO_METERS
                detection.bbox.size.y = dims[1] / CONVERT_SCALE_CM_TO_METERS
                detection.bbox.size.z = dims[2] / CONVERT_SCALE_CM_TO_METERS
                detection_array.detections.append(detection)

                # Draw the cube
                if None not in result['projected_points']:
                    points2d = []
                    for pair in result['projected_points']:
                        points2d.append(tuple(pair))
                    draw.draw_cube(points2d, self.draw_colors[m])

        # Publish the image with results overlaid
        self.pub_rgb_dope_points.publish(
            CvBridge().cv2_to_imgmsg(
                np.array(im)[..., ::-1],
                "bgr8"
            )
        )

        if not depth_msg:
            detection_array_filtered = detection_array
            self.publish_markers(detection_array_filtered)
        else:
            color_unfiltered = ColorRGBA()
            color_unfiltered.r = 1.0
            self.publish_markers(detection_array, color_unfiltered, 'unfiltered_')

            detection_array_filtered = self.post_processing(detection_array, depth_img, camera_matrix)
            color_filtered = ColorRGBA()
            color_filtered.g = 1.0
            self.publish_markers(detection_array_filtered, color_filtered)

            # detection_array_hist = self.post_processing(detection_array, depth_img, camera_matrix,
            #                                             use_histogram=True)
            # color_hist = ColorRGBA()
            # color_hist.b = 1.0
            # self.publish_markers(detection_array_hist, color_hist, 'histogram_')

        self.pub_detections.publish(detection_array_filtered)

    def publish_markers(self, detection_array, color=None, namespace_prefix=''):
        """
        Publishes markers
        :param Detection3DArray detection_array: The detections to visualize
        :param ColorRGBA color: The color to use for all markers. If 'None', use separate color from config for each
                                object class.
        :return: nothing
        """
        # Delete all existing markers
        markers = MarkerArray()
        marker = Marker()
        marker.action = Marker.DELETEALL
        markers.markers.append(marker)
        self.pub_markers.publish(markers)

        # Object markers
        class_id_to_name = {class_id: name for name, class_id in self.class_ids.iteritems()}
        markers = MarkerArray()
        for i, det in enumerate(detection_array.detections):
            name = class_id_to_name[det.results[0].id]
            if color:
                color_rgba = color
            else:
                color_rgba = ColorRGBA()
                draw_color = self.draw_colors[name]
                color_rgba.r = draw_color[0] / 255.0
                color_rgba.g = draw_color[1] / 255.0
                color_rgba.b = draw_color[2] / 255.0
                color_rgba.a = 1.0

            # cube marker
            marker = Marker()
            marker.header = detection_array.header
            marker.action = Marker.ADD
            marker.pose = det.bbox.center
            marker.color = color_rgba
            marker.color.a = 0.4
            marker.ns = namespace_prefix + "bboxes"
            marker.id = i
            marker.type = Marker.CUBE
            marker.scale = det.bbox.size
            markers.markers.append(marker)

            # text marker
            marker = Marker()
            marker.header = detection_array.header
            marker.action = Marker.ADD
            marker.pose = det.bbox.center
            marker.color = color_rgba
            marker.color.a = 1.0
            marker.id = i
            marker.ns = namespace_prefix + "texts"
            marker.type = Marker.TEXT_VIEW_FACING
            marker.scale.x = 0.05
            marker.scale.y = 0.05
            marker.scale.z = 0.05
            marker.text = '{} ({:.2f})'.format(name, det.results[0].score)
            markers.markers.append(marker)

            # mesh marker
            try:
                marker = Marker()
                marker.header = detection_array.header
                marker.action = Marker.ADD
                marker.pose = det.bbox.center
                marker.color = color_rgba
                marker.color.a = 0.7
                marker.ns = namespace_prefix + "meshes"
                marker.id = i
                marker.type = Marker.MESH_RESOURCE
                marker.scale.x = self.mesh_scales[name]
                marker.scale.y = self.mesh_scales[name]
                marker.scale.z = self.mesh_scales[name]
                marker.mesh_resource = self.mesh_urls[name]
                markers.markers.append(marker)
            except KeyError:
                # user didn't specify self.meshes[name], so don't publish marker
                pass

        self.pub_markers.publish(markers)

    def post_processing(self, detection_array, depth_img, camera_matrix, use_histogram=False):
        rospy.logdebug('\n=========================================================\n')

        # serialize and then deserialize the input array to make sure that
        # 1. we're not unintentionally changing the input array (a copy.deepcopy would also have worked)
        # 2. detection_array is completely de-aliased, i.e., no two fields point to the same variable (e.g.
        #    det.bbox.center and det.results[0].pose), otherwise when modifying one of them we unintentionally
        #    also modify the other (copy.deepcopy is not sufficient).
        detection_array = serialize_deserialize(detection_array)

        detection_array_out = Detection3DArray()
        detection_array_out.header = detection_array.header
        camera_intrinsics = CameraIntrinsics(
            frame='camera',
            fx=camera_matrix[(0, 0)],
            fy=camera_matrix[(1, 1)],
            cx=camera_matrix[(0, 2)],
            cy=camera_matrix[(1, 2)],
            skew=camera_matrix[(0, 1)],
            height=depth_img.shape[0],
            width=depth_img.shape[1]
        )

        class_id_to_name = {class_id: name for name, class_id in self.class_ids.iteritems()}
        for i, det in enumerate(detection_array.detections):
            # TODO: filter out detections where bbox has negative z value
            name = class_id_to_name[det.results[0].id]

            quaternion_wxyz = np.array([det.bbox.center.orientation.w,
                                        det.bbox.center.orientation.x,
                                        det.bbox.center.orientation.y,
                                        det.bbox.center.orientation.z])
            translation = np.array([det.bbox.center.position.x,
                                    det.bbox.center.position.y,
                                    det.bbox.center.position.z])
            rotation = RigidTransform.rotation_from_quaternion(quaternion_wxyz)
            object_pose = RigidTransform(
                rotation=rotation,
                translation=translation,
                from_frame='target_model',
                to_frame='camera'
            )

            try:
                mesh = self.meshes[name]
            except KeyError:
                # user didn't specify self.meshes[name], so don't process mesh
                detection_array_out.detections.append(det)
                continue

            synthetic_depth_img = render_depth_image(object_pose, camera_intrinsics, mesh)
            synthetic_depth_img = np.squeeze(synthetic_depth_img.raw_data)

            rospy.logdebug('---------------------------------------------------------')

            # DEPTH_SCALING_FACTOR = 10000.0
            # UNIT_SCALING = 1.0
            # imageio.imwrite('/root/catkin_ws/src/dope/{}_depth.png'.format(i),
            #                 (depth_img * DEPTH_SCALING_FACTOR * UNIT_SCALING).astype(np.uint16))
            # imageio.imwrite('/root/catkin_ws/src/dope/{:d}_synthetic_depth.png'.format(i),
            #                 (synthetic_depth_img * DEPTH_SCALING_FACTOR * UNIT_SCALING).astype(np.uint16))

            # depth_img has nans, synthetic_depth_img has 0.0 for invalid depths
            diffs = depth_img[synthetic_depth_img != 0.0] - synthetic_depth_img[synthetic_depth_img != 0.0]
            diffs = diffs[np.logical_not(np.isnan(diffs))]
            diffs = diffs[np.fabs(diffs) < self.depth_tolerance]

            total_pixels = len(synthetic_depth_img[synthetic_depth_img != 0.0])
            valid_pixels = len(diffs)
            if total_pixels == 0:
                continue
            valid_ratio = float(valid_pixels) / total_pixels
            if valid_ratio <= self.min_valid_points_ratio:
                continue

            if use_histogram:
                histogram, bin_edges = np.histogram(diffs, bins='auto')
                diff = (bin_edges[histogram.argmax()] + bin_edges[histogram.argmax() + 1]) / 2.0
            else:
                diff = diffs.mean()

            rospy.logdebug("id:          {}".format(det.results[0].id))
            rospy.logdebug("valid_ratio: {} ({}/{})".format(valid_ratio, valid_pixels, total_pixels))
            rospy.logdebug("diff:        {}".format(diff))

            det.bbox.center.position.z += diff
            det.results[0].pose.pose.position.z += diff
            detection_array_out.detections.append(det)

        return detection_array_out


def render_depth_image(object_pose, camera_intrinsics, mesh, unit_scaling=1.0, start_viewer=False):
    """
    Renders a synthetic depth image of one object in a given pose.

    :param RigidTransform object_pose:
    :param CameraIntrinsics camera_intrinsics:
    :param trimesh.Trimesh mesh:
    :param float unit_scaling: Scale factor for depth values (e.g. 1.0 = m, 100.0 = cm)
    :param bool start_viewer: Whether to start the graphical scene viewer (for debugging)
    :return: The rendered depth image.
    :rtype: perception.DepthImage
    """
    scene = Scene()

    # Set up a material property (only used for visualization)
    blue_material = MaterialProperties(
        color=np.array([0.1, 0.1, 0.5]),
        k_a=0.3,
        k_d=1.0,
        k_s=1.0,
        alpha=10.0,
        smooth=False
    )
    scene_obj = SceneObject(mesh=mesh, T_obj_world=object_pose, material=blue_material)
    scene.add_object('target_model', scene_obj)

    camera_pose = RigidTransform(from_frame='camera', to_frame='world')
    scene.camera = VirtualCamera(camera_intrinsics, camera_pose, z_near=(0.05 * unit_scaling),
                                 z_far=(6.5535 * unit_scaling))
    if start_viewer:
        SceneViewer(scene, raymond_lighting=True, starting_camera_pose=camera_pose)

    [depth_image] = scene.wrapped_render([RenderMode.DEPTH])

    return depth_image


def rotate_vector(vector, quaternion):
    q_conj = tf.transformations.quaternion_conjugate(quaternion)
    vector = np.array(vector, dtype='float64')
    vector = np.append(vector, [0.0])
    vector = tf.transformations.quaternion_multiply(q_conj, vector)
    vector = tf.transformations.quaternion_multiply(vector, quaternion)
    return vector[:3]


def serialize_deserialize(message):
    """
    Serialize and then deserialize a message. This makes sure that the message is completely flattened and that there
    are no values
    """
    from StringIO import StringIO
    buff = StringIO()
    message.serialize(buff)
    result = message.__class__()  # create new instance of same class as message
    result.deserialize(buff.getvalue())
    return result


def main():
    """Main routine to run DOPE"""

    # Initialize ROS node
    rospy.init_node('dope')
    DopeNode()

    try:
        rospy.spin()
    except rospy.ROSInterruptException:
        pass


if __name__ == "__main__":
    main()
